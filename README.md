# Data Scientist/Analyst

#### Technical Skills: Python, SQL, AWS, Snowflake, MATLAB, Machine Learning, NLP

## Education
- M.S., Data Science & Analytics | Georgia Institute of Technology (_Present_)
- B.S., Computer Science | Kennesaw State University (_May 2022_)
- A.S., Computer Science | Georgia State University (_May 2020_)

## Work Experience
**Data Engineer Intern @ Amazon**
- Contributed to the architecture and implementation of next generation data and BI solutions.  Managed AWS resources including Redshift, S3, EC2, Athena and Glue.
- Collaborated with BI Engineers, Software Engineers, and cross-functionally and delivered high quality data architecture and pipelines. 
- Created big data solutions using EMR/Elastic Search/Redshift. Built efficient and scalable data services and integrated data systems with AWS tools and services to support a variety of customer use cases and applications.
- Configured AWS and Azure cloud infrastructure and managed services through ETL modeling process. Designed, built, and optimized data transformations using SQL in the data modeling layer.

**Data Engineer Intern @ United Parcel Service (UPS)**
- Optimized the companyâ€™s Data Lake and Customer Data Platform (CDP) to support data driven decision making.  Cultivated new data sources and synthesized them into current data lake, enhancing the identity graph as they related to the data.
- Designed and developed ETL processes within CDP in support of the segmentation, analytics, and reporting needs.  Collaborated with IT on development of new ETL jobs, maintenance, and troubleshooting.
- Brought strategy and roadmap to life through data activation processes and oversaw the delivery of high-quality data to marketing.
- Contributed to projects involving marketing stakeholders, software vendors, IT, and data owners to specify and execute new data feeds into the CDP and/or Data Lake from internal repositories and marketing applications such as web analytics, data warehouse, and more. 


**Data Engineer Intern @ Tesla**
- Designed data solutions, optimized data pipeline architecture, as well as developed infrastructure for data collection to support cross-functional teams in the Construction Organization.
- Built the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a variety of data sources.
- Tested multivariate linear & non-linear regression implementations on Hadoop cluster using R, Spark, Mlib, & MATLAB.
- Identified, designed, and implemented internal process improvements: automated manual processes, optimized data delivery, re-designed infrastructure for greater functionality and scalability.




## Projects (All Projects Located in Repository)
### Spotify Music Recommendation System

|In order to find key components for a recommendation system, I carried out exploratory data analysis and exhibited data visualization techniques. I used **Python's** machine learning tools, such sci-kit learn, to create algorithms that could categorize certain variables and properties to get the best suggestion. To import and arrange the substantial Spotify data sets, I also used open-source Python packages like Pandas.|  

- Checked for all the analysis with the target as 'popularity'. Used yellowbrick package to check for the Feature Correlation.
- Performed an analysis by categorizing the data by year to comprehend how the general tone of music has changed throughout time.
- Used the K-means clustering algorithm to divide the genres in the datasets by ten clusters based on the numerical audio features of each genres.

![pandasdata](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/581d3e66-ed12-4aea-bd26-9a55f5abd2ff)


![cluster_graph](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/701bd1c5-ddf0-4fe9-bd3a-2a35ffc19827)





### Credit Card Fraud Detection Project

|This project covers credit card fraud and is meant to look at a dataset of transactions and predict whether it is fraudulent or not. I learned alot of this from Eduonix Learning Solutions. Looking at precision for fraudulent cases lets us know the percentage of cases that they are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying. Our Isolation Forest method was able to produce a better result. Looking at the f1-score 26% of the time we are going to detect the fraudulent transactions.|

**Goal: To get better percentages.**

- You can see most of the V's are clustered around 0 with some or no outliers. Notice we have very few fraudulent cases over valid cases in our class histogram.
- Split the data to assure that I always get the same output and to create reproducible results
- Plotted the histogram of each parameter
- Determined the number fraud classes by creating a data class and and outlier fraction.
- Summarized the large credit card dataset by creating a correlation matrix for the different variables to how all the possible pairs of values in the table are related.

![um yea detect](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/184fe407-121f-414b-827e-ed811975515e)


![Histogram](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/1e18cb5f-874c-4708-b839-c298d11878c9)


![jrgtiuja](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/f2e3d286-6829-4a7e-af55-739bb1fcf3b0)


![graphfrauddetection](https://github.com/AlexusprettyGlass/AlexusGlass.github.io/assets/135679332/58fcd05c-b2cc-46bd-80ac-4d6a8b6230c7)


